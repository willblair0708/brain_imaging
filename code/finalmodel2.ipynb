{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for handling NIfTI files\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, t1w_files, t2w_files, fa_files, adc_files, transform=None):\n",
    "        self.t1w_files = t1w_files\n",
    "        self.t2w_files = t2w_files\n",
    "        self.fa_files = fa_files\n",
    "        # self.adc_files = adc_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t1w_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1w_image = nib.load(self.t1w_files[idx]).get_fdata()\n",
    "        t2w_image = nib.load(self.t2w_files[idx]).get_fdata()\n",
    "        fa_image = nib.load(self.fa_files[idx]).get_fdata()\n",
    "        # adc_image = nib.load(self.adc_files[idx]).get_fdata()\n",
    "\n",
    "        # input_image = np.stack([t1w_image, t2w_image], axis=0)\n",
    "        t1w_image = np.stack([t1w_image], axis=0)\n",
    "        # target_image = np.stack([fa_image, adc_image], axis=0)\n",
    "        t2w_image = np.stack([t2w_image], axis=0)\n",
    "        fa_image = np.stack([fa_image], axis=0)\n",
    "\n",
    "        # if self.transform:\n",
    "            # input_image = self.transform(input_image)\n",
    "            # target_image = self.transform(target_image)\n",
    "\n",
    "        # return input_image, target_image\n",
    "        return t1w_image, t2w_image, fa_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keert\\project2\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001\\\\001\\\\T1w_1mm.nii.gz', '002\\\\002\\\\T1w_1mm.nii.gz', '003\\\\003\\\\T1w_1mm.nii.gz', '004\\\\004\\\\T1w_1mm.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "os.chdir('./data/input')\n",
    "\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder) and folder.startswith('0')]\n",
    "\n",
    "t1w_files = []\n",
    "t2w_files = []\n",
    "fa_files = []\n",
    "adc_files = []\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    path = os.path.join(patient_folder, patient_folder,)\n",
    "\n",
    "    t1w_files.append(os.path.join(path, \"T1w_1mm.nii.gz\"))\n",
    "    t2w_files.append(os.path.join(path, \"T2w_1mm_noalign.nii.gz\"))\n",
    "    adc_files.append(os.path.join(path, \"ADC_deformed.nii.gz\"))\n",
    "    fa_files.append(os.path.join(path, \"FA_deformed.nii.gz\"))\n",
    "\n",
    "    # registered_path = os.path.join(patient_folder, patient_folder, 'registered')\n",
    "    # normalized_path = os.path.join(patient_folder, patient_folder, 'normalized')\n",
    "\n",
    "    # t1w_files.append(os.path.join(normalized_path, \"T1w_1mm_normalized.nii.gz\"))\n",
    "    # t2w_files.append(os.path.join(registered_path, \"T2w_registered.nii.gz\"))\n",
    "    # adc_files.append(os.path.join(registered_path, \"ADC_registered.nii.gz\"))\n",
    "    # fa_files.append(os.path.join(registered_path, \"FA_registered.nii.gz\"))\n",
    "\n",
    "# dataset = BrainDataset(t1w_files, t2w_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "dataset = BrainDataset(fa_files, adc_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) #, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 0 for FA, 1 for ADC\n",
    "output_modality = 0\n",
    "\n",
    "print(t1w_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 145, 174, 145])\n",
      "torch.Size([2, 1, 145, 174, 145])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 145, 174, 145])\n",
      "torch.Size([2, 1, 145, 174, 145])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for inputs, targets in tqdm(dataloader):\n",
    "#     inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#     print(inputs.shape)\n",
    "#     print(targets.shape)\n",
    "\n",
    "for t1, t2, fa in tqdm(dataloader):\n",
    "    t1, t2, fa = t1.to(device), t2.to(device), fa.to(device)\n",
    "\n",
    "    print(t1.shape)\n",
    "    print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 145, 174, 145)\n",
    "\n",
    "nc = 1 # num channels\n",
    "\n",
    "ngf = 32 # size of feature maps in generator\n",
    "\n",
    "ndf = 32 # size of feature maps in discriminator\n",
    "\n",
    "num_epochs = 1 # 200\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "betas = (0.5, 0.999) # beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "ngpu = 1 # number of GPUs available, 0 for CPU mode\n",
    "\n",
    "batch_size = 128 # batch size during training\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "ngpu = 0 # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            # input is 1 x 145 x 174 x 145\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 32 x 72 x 87 x 72\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 64 x 36 x 43 x 36\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            nn.Flatten(),\n",
    "            # input is 128 x 18 x 21 x 18\n",
    "            nn.Linear(128 * 18 * 21 * 18, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.LeakyReLU()\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(), # is this needed?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model = ConvNet()\n",
    "t2_model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "img_shape = (2, 145, 174, 145)\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 128 * 18 * 21 * 18),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d( 128 * 18 * 21 * 18)\n",
    "        )\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layer(x)\n",
    "        x = x.view(x.shape[0], 128, 18, 21, 18) # reshaping tensor\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout3d(0.25),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Dropout3d(0.25),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(64 * 23* 28 * 23, 128),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.Linear(64 * 36 * 43 * 36, 1),\n",
    "            nn.Linear(3855104, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator and generator models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "# generator.apply(weights_init)\n",
    "# discriminator.apply(weights_init)\n",
    "\n",
    "# Define the loss function and optimizer for the discriminator and generator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "al_w = 1\n",
    "generative_loss = nn.MSELoss()\n",
    "gl_w = 1\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_T1 = torch.optim.Adam(t1_model.parameters(), lr=lr*10, betas=betas)\n",
    "optimizer_T2 = torch.optim.Adam(t2_model.parameters(), lr=lr*10, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], Step [0/2], Discriminator Loss: 1.3998, Generator Loss: 39.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:21<00:21, 21.76s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 936537600 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# create latent space and update D with fake image\u001b[39;00m\n\u001b[0;32m     34\u001b[0m t1_latent \u001b[39m=\u001b[39m t1_model(t1\u001b[39m.\u001b[39mfloat()) \n\u001b[1;32m---> 35\u001b[0m t2_latent \u001b[39m=\u001b[39m t2_model(t2\u001b[39m.\u001b[39;49mfloat())\n\u001b[0;32m     36\u001b[0m \u001b[39m# latent = t1_latent + t2_latent\u001b[39;00m\n\u001b[0;32m     37\u001b[0m latent \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat((t1_latent, t2_latent), \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[79], line 27\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvnet(x)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[0;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[0;32m    610\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 936537600 bytes."
     ]
    }
   ],
   "source": [
    "## Change to True / False if you do / do not want \n",
    "## to load the previously saved state dictionaries\n",
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model.pt\"))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "batch_size = 2\n",
    "img_shape = (batch_size, 145, 176, 145)\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(num_epochs):\n",
    "    # # Train the discriminator\n",
    "    # optimizer_D.zero_grad()\n",
    "    i = 0\n",
    "    for t1, t2, fa in tqdm(dataloader):\n",
    "        t1, t2, fa = t1.to(device), t2.to(device), fa.to(device)\n",
    "        \n",
    "        # update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "        real_images = fa.float()\n",
    "        real_labels = torch.ones(real_images.shape[0], 1)#.cuda()\n",
    "        real_predictions = discriminator(real_images)\n",
    "        real_loss = adversarial_loss(real_predictions, real_labels)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # create latent space and update D with fake image\n",
    "        t1_latent = t1_model(t1.float()) \n",
    "        t2_latent = t2_model(t2.float())\n",
    "        # latent = t1_latent + t2_latent\n",
    "        latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(0, 1, 3, 3, 0, 1), mode='replicate') # kinda sketch \n",
    "        fake_labels = torch.zeros(fake_images.shape[0], 1)#.cuda()\n",
    "        fake_predictions = discriminator(fake_images.detach())\n",
    "        fake_loss = adversarial_loss(fake_predictions, fake_labels)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        discriminator_loss = real_loss + fake_loss\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # update G network: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        t1_model.zero_grad()\n",
    "        t2_model.zero_grad()\n",
    "\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(0, 1, 3, 3, 0, 1), mode='replicate') # kinda sketch \n",
    "        fake_predictions = discriminator(fake_images)\n",
    "        errG = adversarial_loss(fake_predictions, real_labels)\n",
    "        errR = generative_loss(fake_images, fa.float())\n",
    "        generator_loss = errG + errR\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()     \n",
    "        optimizer_T1.step()\n",
    "        optimizer_T2.step()   \n",
    "\n",
    "        # output training stats\n",
    "        #if i % 50 == 0:\n",
    "        G_losses.append(generator_loss.item())\n",
    "        D_losses.append(discriminator_loss.item())\n",
    "\n",
    "        # Print the losses\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [%d/%d], Step [%d/%d], Discriminator Loss: %.4f, Generator Loss: %.4f\"\n",
    "                  % (epoch, num_epochs, i, len(dataloader), discriminator_loss.item(), generator_loss.item()))\n",
    "            torch.save(generator.state_dict(), \"generator.pt\")\n",
    "            torch.save(discriminator.state_dict(), \"discriminator.pt\")\n",
    "            torch.save(t1_model.state_dict(), \"t1_model.pt\")\n",
    "            torch.save(t2_model.state_dict(), \"t2_model.pt\")\n",
    "        i += 1\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model.pt\"))\n",
    "\n",
    "def generate_img(t1w_image, t2w_image):\n",
    "    t1w_image = np.stack([t1w_image], axis=0)\n",
    "    t2w_image = np.stack([t2w_image], axis=0)\n",
    "    t1_latent = t1_model(t1w_image.float()) \n",
    "    t2_latent = t2_model(t2w_image.float())\n",
    "    latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "    generated_image = generator(latent)\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "# Change the working directory to the \"data\" folder\n",
    "os.chdir('../data/output')\n",
    "\n",
    "# Get the list of patient folders\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder)]\n",
    "\n",
    "# Register images to T1w space using Diffeomorphic Demons\n",
    "for patient_folder in patient_folders:\n",
    "    output_folder = os.path.abspath(patient_folder)\n",
    "\n",
    "    # Create the output folder for the registered images\n",
    "    synthesized_output_folder = os.path.join(output_folder, \"synthesized\")\n",
    "    os.makedirs(synthesized_output_folder, exist_ok=True)\n",
    "\n",
    "    t1w_image_path = os.path.join(output_folder, \"registered\", \"T1w_1mm_normalized.nii.gz\")\n",
    "    t2w_image_path = os.path.join(output_folder, \"registered\", \"T2w_1mm_normalized.nii.gz\")\n",
    "    fa_image_path = os.path.join(output_folder, \"registered\", \"FA_1.25mm_normalized.nii.gz\")\n",
    "    adc_image_path = os.path.join(output_folder, \"registered\", \"ADC_1.25mm_normalized.nii.gz\")\n",
    "\n",
    "    # synthesized_image_path = os.path.join(synthesized_output_folder, \"ADC_synthesized.nii.gz\")\n",
    "    synthesized_image_path = os.path.join(synthesized_output_folder, \"FA_synthesized.nii.gz\")\n",
    "\n",
    "    t1w_image_file = nib.load(t1w_image_path)\n",
    "    t1w_image = t1w_image_file.get_fdata()\n",
    "    t2w_image_file = nib.load(t2w_image_path)\n",
    "    t2w_image = t2w_image_file.get_fdata()\n",
    "    fa_image_file = nib.load(fa_image_path)\n",
    "    fa_image = fa_image_file.get_fdata()\n",
    "    adc_image_file = nib.load(adc_image_path)\n",
    "    adc_image = adc_image_file.get_fdata()\n",
    "\n",
    "    generated_image = generate_img(t1w_image, t2w_image)\n",
    "\n",
    "    # visualize output vs actual file\n",
    "    plt.rcParams[\"figure.figsize\"]=20,20\n",
    "    plt.figure()\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(generated_image[:,:,100])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(fa_image[:,:,100])\n",
    "    plt.title('Actual Image')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(generated_image[105,:,:])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(fa_image[105,:,:])\n",
    "    plt.title('Actual Image')\n",
    "    plt.show()\n",
    "\n",
    "    sitk.WriteImage(generated_image, synthesized_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mia38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
