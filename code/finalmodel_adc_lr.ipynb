{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for handling NIfTI files\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, t1w_files, t2w_files, fa_files, adc_files, transform=None):\n",
    "        self.t1w_files = t1w_files\n",
    "        self.t2w_files = t2w_files\n",
    "        self.fa_files = fa_files\n",
    "        # self.adc_files = adc_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t1w_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1w_image = nib.load(self.t1w_files[idx]).get_fdata()\n",
    "        t2w_image = nib.load(self.t2w_files[idx]).get_fdata()\n",
    "        # fa_image = nib.load(self.fa_files[idx]).get_fdata()\n",
    "        adc_image = nib.load(self.adc_files[idx]).get_fdata()\n",
    "\n",
    "        # input_image = np.stack([t1w_image, t2w_image], axis=0)\n",
    "        t1w_image = np.stack([t1w_image], axis=0)\n",
    "        # target_image = np.stack([fa_image, adc_image], axis=0)\n",
    "        t2w_image = np.stack([t2w_image], axis=0)\n",
    "        adc_image = np.stack([adc_image], axis=0)\n",
    "\n",
    "        # if self.transform:\n",
    "            # input_image = self.transform(input_image)\n",
    "            # target_image = self.transform(target_image)\n",
    "\n",
    "        # return input_image, target_image\n",
    "        return t1w_image, t2w_image, adc_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/output')\n",
    "\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder) and folder.startswith('0')]\n",
    "\n",
    "t1w_files = []\n",
    "t2w_files = []\n",
    "fa_files = []\n",
    "adc_files = []\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    registered_path = os.path.join(patient_folder, \"registered\")\n",
    "    normalized_path = os.path.join(patient_folder, 'normalized')\n",
    "\n",
    "    t1w_files.append(os.path.join(normalized_path, \"T1w_1mm_normalized.nii.gz\"))\n",
    "    t2w_files.append(os.path.join(registered_path, \"T2w_registered.nii.gz\"))\n",
    "    adc_files.append(os.path.join(registered_path, \"ADC_registered.nii.gz\"))\n",
    "    fa_files.append(os.path.join(registered_path, \"FA_registered.nii.gz\"))\n",
    "\n",
    "    # path = os.path.join(patient_folder, patient_folder,)\n",
    "\n",
    "    # t1w_files.append(os.path.join(path, \"T1w_1mm.nii.gz\"))\n",
    "    # t2w_files.append(os.path.join(path, \"T2w_1mm_noalign.nii.gz\"))\n",
    "    # adc_files.append(os.path.join(path, \"ADC_deformed.nii.gz\"))\n",
    "    # fa_files.append(os.path.join(path, \"FA_deformed.nii.gz\"))\n",
    "\n",
    "dataset = BrainDataset(t1w_files, t2w_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "# dataset = BrainDataset(fa_files, adc_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) #, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 0 for FA, 1 for ADC\n",
    "output_modality = 0\n",
    "\n",
    "print(t1w_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 145, 174, 145)\n",
    "\n",
    "nc = 1 # num channels\n",
    "\n",
    "ngf = 32 # size of feature maps in generator\n",
    "\n",
    "ndf = 32 # size of feature maps in discriminator\n",
    "\n",
    "num_epochs = 1 # 200\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "betas = (0.5, 0.999) # beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "ngpu = 1 # number of GPUs available, 0 for CPU mode\n",
    "\n",
    "batch_size = 128 # batch size during training\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "ngpu = 0 # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            # input is 1 x 145 x 174 x 145\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 32 x 72 x 87 x 72\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 64 x 36 x 43 x 36\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            nn.Flatten(),\n",
    "            # input is 128 x 18 x 21 x 18\n",
    "            nn.Linear(128 * 18 * 21 * 18, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.LeakyReLU()\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(), # is this needed?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model = ConvNet()\n",
    "t2_model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "img_shape = (2, 145, 174, 145)\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 128 * 18 * 21 * 18),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d( 128 * 18 * 21 * 18)\n",
    "        )\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layer(x)\n",
    "        x = x.view(x.shape[0], 128, 18, 21, 18) # reshaping tensor\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout3d(0.25),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Dropout3d(0.25),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(64 * 23* 28 * 23, 128),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.Linear(64 * 36 * 43 * 36, 1),\n",
    "            nn.Linear(3855104, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator and generator models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "# generator.apply(weights_init)\n",
    "# discriminator.apply(weights_init)\n",
    "\n",
    "# Define the loss function and optimizer for the discriminator and generator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "al_w = 1\n",
    "generative_loss = nn.MSELoss()\n",
    "gl_w = 1\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_T1 = torch.optim.Adam(t1_model.parameters(), lr=lr*10, betas=betas)\n",
    "optimizer_T2 = torch.optim.Adam(t2_model.parameters(), lr=lr*10, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change to True / False if you do / do not want \n",
    "## to load the previously saved state dictionaries\n",
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator_adc.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator_adc.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model_adc.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model_adc.pt\"))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "batch_size = 2\n",
    "img_shape = (batch_size, 145, 176, 145)\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(num_epochs):\n",
    "    # # Train the discriminator\n",
    "    # optimizer_D.zero_grad()\n",
    "    i = 0\n",
    "    for t1, t2, adc in tqdm(dataloader):\n",
    "        t1, t2, adc = t1.to(device), t2.to(device), adc.to(device)\n",
    "        \n",
    "        # update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "        real_images = adc.float()\n",
    "        real_labels = torch.ones(real_images.shape[0], 1)#.cuda()\n",
    "        real_predictions = discriminator(real_images)\n",
    "        real_loss = adversarial_loss(real_predictions, real_labels)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # create latent space and update D with fake image\n",
    "        t1_latent = t1_model(t1.float()) \n",
    "        t2_latent = t2_model(t2.float())\n",
    "        # latent = t1_latent + t2_latent\n",
    "        latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(0, 1, 3, 3, 0, 1), mode='replicate') # kinda sketch \n",
    "        fake_labels = torch.zeros(fake_images.shape[0], 1)#.cuda()\n",
    "        fake_predictions = discriminator(fake_images.detach())\n",
    "        fake_loss = adversarial_loss(fake_predictions, fake_labels)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        discriminator_loss = real_loss + fake_loss\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # update G network: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        t1_model.zero_grad()\n",
    "        t2_model.zero_grad()\n",
    "\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(0, 1, 3, 3, 0, 1), mode='replicate') # kinda sketch \n",
    "        fake_predictions = discriminator(fake_images)\n",
    "        errG = adversarial_loss(fake_predictions, real_labels)\n",
    "        errR = generative_loss(fake_images, adc.float())\n",
    "        generator_loss = errG + errR\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()     \n",
    "        optimizer_T1.step()\n",
    "        optimizer_T2.step()   \n",
    "\n",
    "        # output training stats\n",
    "        #if i % 50 == 0:\n",
    "        G_losses.append(generator_loss.item())\n",
    "        D_losses.append(discriminator_loss.item())\n",
    "\n",
    "        # Print the losses\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [%d/%d], Step [%d/%d], Discriminator Loss: %.4f, Generator Loss: %.4f\"\n",
    "                  % (epoch, num_epochs, i, len(dataloader), discriminator_loss.item(), generator_loss.item()))\n",
    "            torch.save(generator.state_dict(), \"generator.pt\")\n",
    "            torch.save(discriminator.state_dict(), \"discriminator.pt\")\n",
    "            torch.save(t1_model.state_dict(), \"t1_model.pt\")\n",
    "            torch.save(t2_model.state_dict(), \"t2_model.pt\")\n",
    "        i += 1\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator_adc.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator_adc.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model_adc.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model_adc.pt\"))\n",
    "\n",
    "def generate_img(t1w_image, t2w_image):\n",
    "    t1w_image = np.stack([t1w_image], axis=0)\n",
    "    t2w_image = np.stack([t2w_image], axis=0)\n",
    "    t1_latent = t1_model(t1w_image.float()) \n",
    "    t2_latent = t2_model(t2w_image.float())\n",
    "    latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "    generated_image = generator(latent)\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "# Change the working directory to the \"data\" folder\n",
    "os.chdir('../data/output')\n",
    "\n",
    "# Get the list of patient folders\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder)]\n",
    "\n",
    "# Register images to T1w space using Diffeomorphic Demons\n",
    "for patient_folder in patient_folders:\n",
    "    output_folder = os.path.abspath(patient_folder)\n",
    "\n",
    "    # Create the output folder for the registered images\n",
    "    synthesized_output_folder = os.path.join(output_folder, \"synthesized\")\n",
    "    os.makedirs(synthesized_output_folder, exist_ok=True)\n",
    "\n",
    "    t1w_image_path = os.path.join(output_folder, \"registered\", \"T1w_1mm_normalized.nii.gz\")\n",
    "    t2w_image_path = os.path.join(output_folder, \"registered\", \"T2w_1mm_normalized.nii.gz\")\n",
    "    fa_image_path = os.path.join(output_folder, \"registered\", \"FA_1.25mm_normalized.nii.gz\")\n",
    "    adc_image_path = os.path.join(output_folder, \"registered\", \"ADC_1.25mm_normalized.nii.gz\")\n",
    "\n",
    "    # synthesized_image_path = os.path.join(synthesized_output_folder, \"ADC_synthesized.nii.gz\")\n",
    "    synthesized_image_path = os.path.join(synthesized_output_folder, \"FA_synthesized.nii.gz\")\n",
    "\n",
    "    t1w_image_file = nib.load(t1w_image_path)\n",
    "    t1w_image = t1w_image_file.get_fdata()\n",
    "    t2w_image_file = nib.load(t2w_image_path)\n",
    "    t2w_image = t2w_image_file.get_fdata()\n",
    "    fa_image_file = nib.load(fa_image_path)\n",
    "    fa_image = fa_image_file.get_fdata()\n",
    "    adc_image_file = nib.load(adc_image_path)\n",
    "    adc_image = adc_image_file.get_fdata()\n",
    "\n",
    "    generated_image = generate_img(t1w_image, t2w_image)\n",
    "\n",
    "    # visualize output vs actual file\n",
    "    plt.rcParams[\"figure.figsize\"]=20,20\n",
    "    plt.figure()\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(generated_image[:,:,100])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(adc_image[:,:,100])\n",
    "    plt.title('Actual Image')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(generated_image[105,:,:])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(adc_image[105,:,:])\n",
    "    plt.title('Actual Image')\n",
    "    plt.show()\n",
    "\n",
    "    sitk.WriteImage(generated_image, synthesized_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mia38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
