{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "# from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv3DTranspose, BatchNormalization, LeakyReLU, concatenate\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # Define the input shape\n",
    "# input_shape = (160, 160, 192, 1)\n",
    "\n",
    "# # Create a sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add convolutional layers with max pooling\n",
    "# model.add(Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "# model.add(Conv3D(64, (3, 3, 3), activation='relu'))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "# model.add(Conv3D(128, (3, 3, 3), activation='relu'))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# # Add fully connected layers\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Define the input shape\n",
    "# img_shape = (160, 160, 192, 2)\n",
    "# latent_dim = 100\n",
    "\n",
    "# # Create a generator model\n",
    "# generator = Sequential()\n",
    "\n",
    "# generator.add(Dense(128 * 20 * 20 * 24, activation=\"relu\", input_dim=latent_dim))\n",
    "# generator.add(Reshape((20, 20, 24, 128)))\n",
    "# generator.add(Conv3DTranspose(128, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\"))\n",
    "# generator.add(BatchNormalization(momentum=0.8))\n",
    "# generator.add(LeakyReLU(alpha=0.2))\n",
    "# generator.add(Conv3DTranspose(64, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\"))\n",
    "# generator.add(BatchNormalization(momentum=0.8))\n",
    "# generator.add(LeakyReLU(alpha=0.2))\n",
    "# generator.add(Conv3DTranspose(2, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\", activation='tanh'))\n",
    "\n",
    "# # Create a discriminator model\n",
    "# discriminator = Sequential()\n",
    "\n",
    "# discriminator.add(Conv3D(32, kernel_size=(3, 3, 3), strides=(2, 2, 2), input_shape=img_shape, padding=\"same\"))\n",
    "# discriminator.add(LeakyReLU(alpha=0.2))\n",
    "# discriminator.add(Dropout(0.25))\n",
    "# discriminator.add(Conv3D(64, kernel_size=(3, 3, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for handling NIfTI files\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, t1w_files, t2w_files, fa_files, adc_files, transform=None):\n",
    "        self.t1w_files = t1w_files\n",
    "        self.t2w_files = t2w_files\n",
    "        self.fa_files = fa_files\n",
    "        self.adc_files = adc_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t1w_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1w_image = nib.load(self.t1w_files[idx]).get_fdata()\n",
    "        t2w_image = nib.load(self.t2w_files[idx]).get_fdata()\n",
    "        fa_image = nib.load(self.fa_files[idx]).get_fdata()\n",
    "        adc_image = nib.load(self.adc_files[idx]).get_fdata()\n",
    "\n",
    "        input_image = np.stack([t1w_image, t2w_image], axis=0)\n",
    "        target_image = np.stack([fa_image, adc_image], axis=0)\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "os.chdir(root_dir)\n",
    "# Change the working directory to the \"data\" folder\n",
    "os.chdir('../data/output')\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder)]\n",
    "\n",
    "t1w_files = []\n",
    "t2w_files = []\n",
    "fa_files = []\n",
    "adc_files = []\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    registered_path = os.path.join(patient_folder, 'registered')\n",
    "    normalized_path = os.path.join(patient_folder, 'normalized')\n",
    "\n",
    "    t1w_files.append(os.path.join(normalized_path, \"T1w_1mm_normalized.nii.gz\"))\n",
    "    t2w_files.append(os.path.join(registered_path, \"T2w_registered.nii.gz\"))\n",
    "    adc_files.append(os.path.join(registered_path, \"ADC_registered.nii.gz\"))\n",
    "    fa_files.append(os.path.join(registered_path, \"FA_registered.nii.gz\"))\n",
    "\n",
    "dataset = BrainDataset(t1w_files, t2w_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 0 for FA, 1 for ADC\n",
    "output_modality = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['055\\\\normalized\\\\T1w_1mm_normalized.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(t1w_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\keert\\\\project2\\\\data\\\\output'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     plotting\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m t1w_files:\n\u001b[1;32m---> 19\u001b[0m     display_image(file,\u001b[39m'\u001b[39;49m\u001b[39mT1w\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mdisplay_image\u001b[1;34m(input_image_file, title)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisplay_image\u001b[39m(input_image_file, title):\n\u001b[0;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m    Display NIfTI image using nilearn.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m        title (str): Title of the plot.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     plotting\u001b[39m.\u001b[39;49mplot_anat(input_image_file, title\u001b[39m=\u001b[39;49mtitle \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m Original\u001b[39;49m\u001b[39m\"\u001b[39;49m, display_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mortho\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     16\u001b[0m     plotting\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\plotting\\img_plotting.py:463\u001b[0m, in \u001b[0;36mplot_anat\u001b[1;34m(anat_img, cut_coords, output_file, display_mode, figure, axes, title, annotate, threshold, draw_cross, black_bg, dim, cmap, colorbar, cbar_tick_format, vmin, vmax, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m@fill_doc\u001b[39m\n\u001b[0;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_anat\u001b[39m(anat_img\u001b[39m=\u001b[39mMNI152TEMPLATE, cut_coords\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    415\u001b[0m               output_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, display_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mortho\u001b[39m\u001b[39m'\u001b[39m, figure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m               colorbar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, cbar_tick_format\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%.2g\u001b[39;00m\u001b[39m\"\u001b[39m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    419\u001b[0m               vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    420\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Plot cuts of an anatomical image (by default 3 cuts:\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m    Frontal, Axial, and Lateral)\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m \n\u001b[0;32m    462\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m     anat_img, black_bg, anat_vmin, anat_vmax \u001b[39m=\u001b[39m _load_anat(\n\u001b[0;32m    464\u001b[0m         anat_img,\n\u001b[0;32m    465\u001b[0m         dim\u001b[39m=\u001b[39;49mdim, black_bg\u001b[39m=\u001b[39;49mblack_bg)\n\u001b[0;32m    467\u001b[0m     \u001b[39mif\u001b[39;00m vmin \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         vmin \u001b[39m=\u001b[39m anat_vmin\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\plotting\\img_plotting.py:374\u001b[0m, in \u001b[0;36m_load_anat\u001b[1;34m(anat_img, dim, black_bg)\u001b[0m\n\u001b[0;32m    372\u001b[0m         black_bg \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     anat_img \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mcheck_niimg_3d(anat_img)\n\u001b[0;32m    375\u001b[0m     \u001b[39m# Clean anat_img for non-finite values to avoid computing unnecessary\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[39m# border data values.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     data \u001b[39m=\u001b[39m _safe_get_data(anat_img, ensure_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\_utils\\niimg_conversions.py:371\u001b[0m, in \u001b[0;36mcheck_niimg_3d\u001b[1;34m(niimg, dtype)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_niimg_3d\u001b[39m(niimg, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    338\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that niimg is a proper 3D niimg-like object and load it.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[0;32m    340\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mreturn\u001b[39;00m check_niimg(niimg, ensure_ndim\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\_utils\\niimg_conversions.py:316\u001b[0m, in \u001b[0;36mcheck_niimg\u001b[1;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[39mreturn\u001b[39;00m concat_niimgs(niimg, ensure_ndim\u001b[39m=\u001b[39mensure_ndim, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    315\u001b[0m \u001b[39m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m niimg \u001b[39m=\u001b[39m load_niimg(niimg, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    318\u001b[0m \u001b[39mif\u001b[39;00m ensure_ndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(niimg\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m niimg\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    319\u001b[0m     \u001b[39m# \"squeeze\" the image.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     data \u001b[39m=\u001b[39m _safe_get_data(niimg)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\_utils\\niimg.py:134\u001b[0m, in \u001b[0;36mload_niimg\u001b[1;34m(niimg, dtype)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(niimg, nibabel\u001b[39m.\u001b[39mspatialimages\u001b[39m.\u001b[39mSpatialImage):\n\u001b[0;32m    128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData given cannot be loaded because it is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m not compatible with nibabel format:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[39m+\u001b[39m _repr_niimgs(niimg, shorten\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m     )\n\u001b[1;32m--> 134\u001b[0m dtype \u001b[39m=\u001b[39m _get_target_dtype(_get_data(niimg)\u001b[39m.\u001b[39mdtype, dtype)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Copyheader and set dtype in header if header exists\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m niimg\u001b[39m.\u001b[39mheader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nilearn\\_utils\\niimg.py:25\u001b[0m, in \u001b[0;36m_get_data\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39m_data_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39m_data_cache\n\u001b[1;32m---> 25\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(img\u001b[39m.\u001b[39;49m_dataobj)\n\u001b[0;32m     26\u001b[0m img\u001b[39m.\u001b[39m_data_cache \u001b[39m=\u001b[39m data\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nibabel\\arrayproxy.py:439\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    419\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[39m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_scaled(dtype\u001b[39m=\u001b[39;49mdtype, slicer\u001b[39m=\u001b[39;49m())\n\u001b[0;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m         arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nibabel\\arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    404\u001b[0m     scl_inter \u001b[39m=\u001b[39m scl_inter\u001b[39m.\u001b[39mastype(use_dtype)\n\u001b[0;32m    405\u001b[0m \u001b[39m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m scaled \u001b[39m=\u001b[39m apply_read_scaling(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_unscaled(slicer\u001b[39m=\u001b[39;49mslicer), scl_slope, scl_inter)\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     scaled \u001b[39m=\u001b[39m scaled\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mpromote_types(scaled\u001b[39m.\u001b[39mdtype, dtype), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nibabel\\arrayproxy.py:376\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[1;34m(self, slicer)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m canonical_slicers(slicer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape, \u001b[39mFalse\u001b[39;00m) \u001b[39m==\u001b[39m canonical_slicers(\n\u001b[0;32m    373\u001b[0m     (), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    374\u001b[0m ):\n\u001b[0;32m    375\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 376\u001b[0m         \u001b[39mreturn\u001b[39;00m array_from_file(\n\u001b[0;32m    377\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shape,\n\u001b[0;32m    378\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype,\n\u001b[0;32m    379\u001b[0m             fileobj,\n\u001b[0;32m    380\u001b[0m             offset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_offset,\n\u001b[0;32m    381\u001b[0m             order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morder,\n\u001b[0;32m    382\u001b[0m             mmap\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mmap,\n\u001b[0;32m    383\u001b[0m         )\n\u001b[0;32m    384\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj:\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m fileslice(\n\u001b[0;32m    386\u001b[0m         fileobj,\n\u001b[0;32m    387\u001b[0m         slicer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m         lock\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock,\n\u001b[0;32m    393\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia\\Lib\\site-packages\\nibabel\\volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[1;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(infile, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    464\u001b[0m     data_bytes \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(n_bytes)\n\u001b[1;32m--> 465\u001b[0m     n_read \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39;49mreadinto(data_bytes)\n\u001b[0;32m    466\u001b[0m     needs_copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mread(size)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\gzip.py:518\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mif\u001b[39;00m buf \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompressed file ended before the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mend-of-stream marker was reached\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_read_data( uncompress )\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(uncompress)\n",
      "\u001b[1;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "from nilearn import plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "def display_image(input_image_file, title):\n",
    "    \"\"\"\n",
    "    Display NIfTI image using nilearn.\n",
    "    \n",
    "    Args:\n",
    "        input_image_file (str): Path to the original NIfTI file.\n",
    "        output_image_file (str): Path to the skull-stripped NIfTI file.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plotting.plot_anat(input_image_file, title=title + \" Original\", display_mode='ortho')\n",
    "    plotting.show()\n",
    "\n",
    "for file in t1w_files:\n",
    "    display_image(file,'T1w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 182, 218, 182)\n",
    "\n",
    "nc = 1 # num channels\n",
    "\n",
    "ngf = 32 # size of feature maps in generator\n",
    "\n",
    "ndf = 32 # size of feature maps in discriminator\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "betas = (0.5, 0.999) # beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "ngpu = 1 # number of GPUs available, 0 for CPU mode\n",
    "\n",
    "batch_size = 128 # batch size during training\n",
    "\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            # input is 1 x 182 x 218 x 182\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 32 x 91 x 109 x 91\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 64 x 46 x 55 x 46\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 128 x 23 x 28 x 23\n",
    "            nn.Linear(128 * 23 * 28 * 23, latent_dim),\n",
    "            nn.BatchNorm3d(latent_dim),\n",
    "            nn.LeakyReLU()\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(), # is this needed?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model = ConvNet()\n",
    "t2_model = ConvNet()\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# randomly initializing weights\n",
    "t1_model.apply(weights_init)\n",
    "t2_model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "img_shape = (2, 160, 160, 192)\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 128 * 23 * 28 * 23),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d( 128 * 23 * 28 * 23)\n",
    "        )\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(32, 2, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layer(x)\n",
    "        x = x.view(x.shape[0], 128, 23, 28, 23) # reshaping tensor\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout3d(0.25),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Dropout3d(0.25),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(64 * 23* 28 * 23, 128),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(64, 1),\n",
    "            nn.Linear(64 * 23 * 28 * 23, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator and generator models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# Define the loss function and optimizer for the discriminator and generator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generative_loss = nn.MSELoss()\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_T1 = torch.optim.Adam(t1_model.parameters(), lr=lr*10, betas=betas)\n",
    "optimizer_T2 = torch.optim.Adam(t2_model.parameters(), lr=lr*10, betas=betas)\n",
    "\n",
    "# Define the input shape for the real and fake images\n",
    "real_imgs = torch.zeros((32, *input_shape))\n",
    "fake_imgs = torch.zeros((32, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change to True / False if you do / do not want \n",
    "## to load the previously saved state dictionaries\n",
    "load = True\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator.pt\"))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(num_epochs):\n",
    "    # # Train the discriminator\n",
    "    # optimizer_D.zero_grad()\n",
    "    for i, inputs, targets in enumerate(tqdm(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "        real_image = targets[output_modality]\n",
    "        real_label = torch.ones(real_image.shape[0], 1).cuda()\n",
    "        real_prediction = discriminator(real_image)\n",
    "        real_loss = adversarial_loss(real_prediction, real_label)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # create latent space and update D with fake image\n",
    "        t1_latent = t1_model(inputs[0]) \n",
    "        t2_latent = t2_model(inputs[1])\n",
    "        # TODO: change to concat\n",
    "        # latent = t1_latent + t2_latent\n",
    "        latent = torch.concat((t1_latent, t2_latent), 0) # might have to change 0 to 1\n",
    "        fake_image = generator(latent)\n",
    "        fake_labels = torch.zeros(fake_image.shape[0], 1).cuda()\n",
    "        fake_predictions = discriminator(fake_image.detach())\n",
    "        fake_loss = adversarial_loss(fake_predictions, fake_labels)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        discriminator_loss = real_loss + fake_loss\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # update G network: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        t1_model.zero_grad()\n",
    "        t2_model.zero_grad()\n",
    "\n",
    "        fake = generator(latent)\n",
    "        fake_prediction = discriminator(fake)\n",
    "        errG = adversarial_loss(fake_prediction, real_label)\n",
    "        errR = generative_loss(fake, targets[output_modality])\n",
    "        generator_loss = errG + errR\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()     \n",
    "        optimizer_T1.step()\n",
    "        optimizer_T2.step()   \n",
    "\n",
    "        # output training stats\n",
    "        #if i % 50 == 0:\n",
    "        G_losses.append(generator_loss.item())\n",
    "        D_losses.append(discriminator_loss.item())\n",
    "\n",
    "        # Print the losses\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [%d/%d], Step [%d/%d], Discriminator Loss: %.4f, Generator Loss: %.4f\"\n",
    "                  % (epoch, num_epochs, i, len(dataloader), discriminator_loss.item(), generator_loss.item()))\n",
    "            torch.save(generator.state_dict(), \"generator.pt\")\n",
    "            torch.save(discriminator.state_dict(), \"discriminator.pt\")\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
