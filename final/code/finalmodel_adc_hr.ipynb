{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for handling NIfTI files\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, t1w_files, t2w_files, fa_files, adc_files, transform=None):\n",
    "        self.t1w_files = t1w_files\n",
    "        self.t2w_files = t2w_files\n",
    "        # self.fa_files = fa_files\n",
    "        self.adc_files = adc_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t1w_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1w_image = nib.load(self.t1w_files[idx]).get_fdata()\n",
    "        t2w_image = nib.load(self.t2w_files[idx]).get_fdata()\n",
    "        # fa_image = nib.load(self.fa_files[idx]).get_fdata()\n",
    "        adc_image = nib.load(self.adc_files[idx]).get_fdata()\n",
    "\n",
    "        # input_image = np.stack([t1w_image, t2w_image], axis=0)\n",
    "        t1w_image = np.stack([t1w_image], axis=0)\n",
    "        # target_image = np.stack([fa_image, adc_image], axis=0)\n",
    "        t2w_image = np.stack([t2w_image], axis=0)\n",
    "        adc_image = np.stack([adc_image], axis=0)\n",
    "\n",
    "        # if self.transform:\n",
    "            # input_image = self.transform(input_image)\n",
    "            # target_image = self.transform(target_image)\n",
    "\n",
    "        # return input_image, target_image\n",
    "        return t1w_image, t2w_image, adc_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keert\\project2\n"
     ]
    }
   ],
   "source": [
    "cd project2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keert\\project2\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001\\\\001\\\\T1w_1mm.nii.gz', '002\\\\002\\\\T1w_1mm.nii.gz', '003\\\\003\\\\T1w_1mm.nii.gz', '004\\\\004\\\\T1w_1mm.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('./data/input')\n",
    "os.chdir('./data/output')\n",
    "\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder) and folder.startswith('0')]\n",
    "\n",
    "t1w_files = []\n",
    "t2w_files = []\n",
    "fa_files = []\n",
    "adc_files = []\n",
    "\n",
    "for patient_folder in patient_folders:\n",
    "    registered_path = os.path.join(patient_folder, \"registered\")\n",
    "    registered2_path = os.path.join(patient_folder, \"registered2\")\n",
    "    normalized_path = os.path.join(patient_folder, 'normalized')\n",
    "\n",
    "    t1w_files.append(os.path.join(normalized_path, \"T1w_1mm_normalized.nii.gz\"))\n",
    "    t2w_files.append(os.path.join(registered_path, \"T2w_registered.nii.gz\"))\n",
    "    adc_files.append(os.path.join(registered2_path, \"ADC_registered2.nii.gz\"))\n",
    "    fa_files.append(os.path.join(registered2_path, \"FA_registered2.nii.gz\"))\n",
    "\n",
    "    # path = os.path.join(patient_folder, patient_folder)\n",
    "\n",
    "    # t1w_files.append(os.path.join(path, \"T1w_1mm.nii.gz\"))\n",
    "    # t2w_files.append(os.path.join(path, \"T2w_1mm_noalign.nii.gz\"))\n",
    "    # adc_files.append(os.path.join(path, \"ADC_deformed.nii.gz\"))\n",
    "    # fa_files.append(os.path.join(path, \"FA_deformed.nii.gz\"))\n",
    "\n",
    "dataset = BrainDataset(t1w_files, t2w_files, fa_files, adc_files, transform=Compose([torch.tensor]))\n",
    "# dataset = BrainDataset(t1w_files, t2w_files, t1w_files, t2w_files, transform=Compose([torch.tensor]))\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 0 for FA, 1 for ADC\n",
    "output_modality = 1\n",
    "\n",
    "print(t1w_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 182, 218, 182)\n",
    "\n",
    "nc = 1 # num channels\n",
    "\n",
    "ngf = 32 # size of feature maps in generator\n",
    "\n",
    "ndf = 32 # size of feature maps in discriminator\n",
    "\n",
    "num_epochs = 1 # 200\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "betas = (0.5, 0.999) # beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "ngpu = 1 # number of GPUs available, 0 for CPU mode\n",
    "\n",
    "batch_size = 128 # batch size during training\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "ngpu = 1 # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            # input is 1 x 182 x 218 x 182\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 32 x 91 x 109 x 91\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            # input is 64 x 45 x 54 x 45\n",
    "            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
    "            nn.Flatten(),\n",
    "            # input is 128 x 22 x 27 x 22\n",
    "            nn.Linear(1672704, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.LeakyReLU()\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(), # is this needed?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_model = ConvNet()\n",
    "t2_model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "img_shape = (2, 182, 218, 182)\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 128 * 22 * 27 * 22),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128 * 22 * 27 * 22)\n",
    "        )\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose3d(32, 1, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layer(x)\n",
    "        x = x.view(x.shape[0], 128, 22, 27, 22) # reshaping tensor\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout3d(0.25),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #nn.Dropout3d(0.25),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(64 * 23* 28 * 23, 128),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(0.2), #inplace=True\n",
    "            # nn.Linear(64, 1),\n",
    "            # nn.Linear(64 * 36 * 43 * 36, 1),\n",
    "            nn.Linear(7448320, 1), # 64 x 45 x 54 x 45\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator and generator models\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "\n",
    "# generator.apply(weights_init)\n",
    "# discriminator.apply(weights_init)\n",
    "\n",
    "# Define the loss function and optimizer for the discriminator and generator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "al_w = 1\n",
    "generative_loss = nn.MSELoss()\n",
    "gl_w = 0.1\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_T1 = torch.optim.Adam(t1_model.parameters(), lr=lr*10, betas=betas)\n",
    "optimizer_T2 = torch.optim.Adam(t2_model.parameters(), lr=lr*10, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1], Step [0/2], Discriminator Loss: 2.0071, Generator Loss: 4588.7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:48<01:48, 108.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m real_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m \u001b[39m# create latent space and update D with fake image\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m t1_latent \u001b[39m=\u001b[39m t1_model(t1\u001b[39m.\u001b[39;49mfloat()) \n\u001b[0;32m     37\u001b[0m t2_latent \u001b[39m=\u001b[39m t2_model(t2\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     38\u001b[0m \u001b[39m# latent = t1_latent + t2_latent\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvnet(x)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\keert\\project2\\mia38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[0;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[0;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[0;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[0;32m    610\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Change to True / False if you do / do not want \n",
    "## to load the previously saved state dictionaries\n",
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model.pt\"))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "batch_size = 2\n",
    "img_shape = (batch_size, 182, 218, 182)\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(num_epochs):\n",
    "    # # Train the discriminator\n",
    "    # optimizer_D.zero_grad()\n",
    "    i = 0\n",
    "    for t1, t2, fa in tqdm(dataloader):\n",
    "        t1, t2, fa = t1.to(device), t2.to(device), fa.to(device)\n",
    "        \n",
    "        # update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "        real_images = fa.float()\n",
    "        real_labels = torch.ones(real_images.shape[0], 1).cuda()\n",
    "        real_predictions = discriminator(real_images)\n",
    "        real_loss = adversarial_loss(real_predictions, real_labels)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # create latent space and update D with fake image\n",
    "        t1_latent = t1_model(t1.float()) \n",
    "        t2_latent = t2_model(t2.float())\n",
    "        # latent = t1_latent + t2_latent\n",
    "        latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(3, 3, 1, 1, 3, 3), mode='replicate') # kinda sketch \n",
    "        fake_labels = torch.zeros(fake_images.shape[0], 1).cuda()\n",
    "        fake_predictions = discriminator(fake_images.detach())\n",
    "        fake_loss = adversarial_loss(fake_predictions, fake_labels)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        discriminator_loss = real_loss + fake_loss\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # update G network: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        t1_model.zero_grad()\n",
    "        t2_model.zero_grad()\n",
    "\n",
    "        fake_images = generator(latent)\n",
    "        fake_images = torch.nn.functional.pad(fake_images, pad=(3, 3, 1, 1, 3, 3), mode='replicate') # kinda sketch \n",
    "        fake_predictions = discriminator(fake_images)\n",
    "        errG = adversarial_loss(fake_predictions, real_labels)\n",
    "        errR = generative_loss(fake_images, fa.float())\n",
    "        generator_loss = al_w*errG + gl_w*errR\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()     \n",
    "        optimizer_T1.step()\n",
    "        optimizer_T2.step()   \n",
    "\n",
    "        # output training stats\n",
    "        #if i % 50 == 0:\n",
    "        G_losses.append(generator_loss.item())\n",
    "        D_losses.append(discriminator_loss.item())\n",
    "\n",
    "        # Print the losses\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [%d/%d], Step [%d/%d], Discriminator Loss: %.4f, Generator Loss: %.4f\"\n",
    "                  % (epoch, num_epochs, i, len(dataloader), discriminator_loss.item(), generator_loss.item()))\n",
    "            torch.save(generator.state_dict(), \"generator.pt\")\n",
    "            torch.save(discriminator.state_dict(), \"discriminator.pt\")\n",
    "            torch.save(t1_model.state_dict(), \"t1_model.pt\")\n",
    "            torch.save(t2_model.state_dict(), \"t2_model.pt\")\n",
    "        i += 1\n",
    "            \n",
    "        iters += 1\n",
    "\n",
    "        # t1_latent = t1_model(t1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mGenerator and Discriminator Loss During Training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mplot(G_losses,label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mG\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(D_losses,label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39miterations\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "if load:\n",
    "    generator.load_state_dict(torch.load(\"generator_fa.pt\"))\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator_fa.pt\"))\n",
    "    t1_model.load_state_dict(torch.load(\"t1_model_fa.pt\"))\n",
    "    t2_model.load_state_dict(torch.load(\"t2_model_fa.pt\"))\n",
    "\n",
    "def generate_img(t1w_image, t2w_image):\n",
    "    t1w_image = np.stack([t1w_image], axis=0)\n",
    "    t2w_image = np.stack([t2w_image], axis=0)\n",
    "    t1_latent = t1_model(t1w_image.float()) \n",
    "    t2_latent = t2_model(t2w_image.float())\n",
    "    latent = torch.concat((t1_latent, t2_latent), 1)\n",
    "    generated_image = generator(latent)\n",
    "    generated_image = torch.nn.functional.pad(generated_image, pad=(3, 3, 1, 1, 3, 3), mode='replicate') # kinda sketch \n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "# Change the working directory to the \"data\" folder\n",
    "os.chdir('../data/output')\n",
    "\n",
    "# Get the list of patient folders\n",
    "patient_folders = [folder for folder in os.listdir() if os.path.isdir(folder)]\n",
    "\n",
    "# Register images to T1w space using Diffeomorphic Demons\n",
    "for patient_folder in patient_folders:\n",
    "    output_folder = os.path.abspath(patient_folder)\n",
    "\n",
    "    # Create the output folder for the registered images\n",
    "    synthesized_output_folder = os.path.join(output_folder, \"synthesized\")\n",
    "    os.makedirs(synthesized_output_folder, exist_ok=True)\n",
    "\n",
    "    t1w_image_path = os.path.join(output_folder, \"normalized\", \"T1w_1mm_normalized.nii.gz\")\n",
    "    t2w_image_path = os.path.join(output_folder, \"registered\", \"T2w_registered.nii.gz\")\n",
    "    fa_image_path = os.path.join(output_folder, \"registered2\", \"FA_registered.nii.gz\")\n",
    "    adc_image_path = os.path.join(output_folder, \"registered2\", \"ADC_registered.nii.gz\")\n",
    "\n",
    "    # synthesized_image_path = os.path.join(synthesized_output_folder, \"ADC_synthesized.nii.gz\")\n",
    "    synthesized_image_path = os.path.join(synthesized_output_folder, \"FA_synthesized.nii.gz\")\n",
    "\n",
    "    t1w_image_file = nib.load(t1w_image_path)\n",
    "    t1w_image = t1w_image_file.get_fdata()\n",
    "    t2w_image_file = nib.load(t2w_image_path)\n",
    "    t2w_image = t2w_image_file.get_fdata()\n",
    "    fa_image_file = nib.load(fa_image_path)\n",
    "    fa_image = fa_image_file.get_fdata()\n",
    "    adc_image_file = nib.load(adc_image_path)\n",
    "    adc_image = adc_image_file.get_fdata()\n",
    "\n",
    "    generated_image = generate_img(t1w_image, t2w_image)\n",
    "\n",
    "    # visualize output vs actual file\n",
    "    plt.rcParams[\"figure.figsize\"]=20,20\n",
    "    plt.figure()\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(generated_image[:,:,100])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(adc_image[:,:,100])\n",
    "    plt.title('Actual Image')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(generated_image[105,:,:])\n",
    "    plt.title('Generated Image')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(adc_image[105,:,:])\n",
    "    plt.title('Actual Image')\n",
    "    plt.show()\n",
    "\n",
    "    sitk.WriteImage(generated_image, synthesized_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mia38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
